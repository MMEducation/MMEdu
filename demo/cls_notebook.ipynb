{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片分类模块MMClassification快速入门\n",
    "\n",
    "## 1.简介\n",
    "\n",
    "`MMClassification模块`主要功能是对图片进行分类，内置了常见的图片分类网络模型，有`LeNet`、`MobilNet`和`ResNet50`，并提供了预训练模型。\n",
    "\n",
    "## 2.导入基础库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2'\n"
     ]
    }
   ],
   "source": [
    "from base import *\n",
    "from MMEdu import MMClassification as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型推理\n",
    "\n",
    "我们提供了常见模型的预训练模型，借助预训练模型就能体验AI的推理过程。\n",
    "\n",
    "#### 第一步：实例化模型\n",
    "\n",
    "`MMClassification`推荐的网络模型是`LeNet`，这是一个著名的轻量级卷积神经网络模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cls('LeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二步：指定模型保存的路径\n",
    "\n",
    "训练好的模型包含两个文件，一个是权重文件，扩展名是.pth，另一个是分类标签信息文件，用来指定训练集的路径，代表训练集中所包含的所有类别，扩展名是.txt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '../checkpoints/cls_model/hand_gray/latest.pth'\n",
    "class_path = '../dataset/cls/hand_gray/classes.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三步：指定图片，开始推理\n",
    "\n",
    "我们提供的`LeNet`预训练模型是基于经典的“剪刀石头布”数据集训练的，你可以找几张剪刀石头布的手势的图片，让AI“识别”一下。\n",
    "`show`代表是否需要显示结果图片，设置为`True`表示推理后显示推理结果图。\n",
    "\n",
    "在进行`inference`推理之后我们可以使用`model.print_result()`函数进行输出，通常来说我们会在输出结果里阐述各个数据代表的意义，同时也会将输出内容进行中文化处理，方便更多的受众群体能够理解人工智能推理所带来的结果数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= begin inference ==========\n",
      "load checkpoint from local path: ../checkpoints/cls_model/hand_gray/latest.pth\n",
      "========= finish inference ==========\n",
      "检测结果如下：\n",
      "[{'标签': 1, '置信度': 0.6409855484962463, '预测结果': 'rock'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'标签': 1, '置信度': 0.6409855484962463, '预测结果': 'rock'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 128.01x128.01 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = 'testrock01-02.png'\n",
    "result = model.inference(image=img, show=True, class_path=class_path,checkpoint = checkpoint)\n",
    "model.print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时您会发现当前目录下出现了一个新的文件夹“cls_result”，里面放着这张图片的推理结果图，名称为原本的图片文件名称，下次推理的结果图也会放到这个文件夹。可以得到这个文件夹下查看。\n",
    "\n",
    "返回的数据类型是一个字典列表（很多个字典组成的列表）类型的变量，内置的字典表示分类的结果，如“`{'标签': 1, '置信度': 0.6409855484962463, '预测结果': 'rock'}`”，我们可以用字典访问其中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测结果如下：\n",
      "[{'标签': 1, '置信度': 0.6409855484962463, '预测结果': 'rock'}]\n",
      "标签（序号）为： 1\n",
      "识别结果为： 石头\n"
     ]
    }
   ],
   "source": [
    "x = model.print_result(result)\n",
    "print('标签（序号）为：',x[0]['标签'])\n",
    "classes=['布','石头','剪刀']\n",
    "print('识别结果为：',classes[x[0]['标签']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第四步：指定一组图片，开始推理\n",
    "您也可以将收集的图片放在一个文件夹下，然后指定文件夹路径进行一组图片的批量推理。如在demo文件夹下新建一个cls_testIMG文件夹放图片，运行下面这段代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= begin inference ==========\n",
      "load checkpoint from local path: ../checkpoints/cls_model/hand_gray/latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= finish inference ==========\n",
      "检测结果如下：\n",
      "[[{'标签': 0, '置信度': 0.7293426394462585, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.9959233999252319, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.9995439648628235, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.9937105178833008, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.9985081553459167, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.7106857895851135, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.8624805212020874, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.9401789903640747, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.9914982914924622, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.7505924105644226, '预测结果': 'paper'}], [{'标签': 0, '置信度': 0.5732581615447998, '预测结果': 'paper'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'标签': 0, '置信度': 0.7293426394462585, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.9959233999252319, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.9995439648628235, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.9937105178833008, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.9985081553459167, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.7106857895851135, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.8624805212020874, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.9401789903640747, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.9914982914924622, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.7505924105644226, '预测结果': 'paper'}],\n",
       " [{'标签': 0, '置信度': 0.5732581615447998, '预测结果': 'paper'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '../dataset/cls/hand_gray/val_set/paper/'\n",
    "model = cls('LeNet')\n",
    "checkpoint = '../checkpoints/cls_model/hand_gray/latest.pth'\n",
    "class_path = '../dataset/cls/hand_gray/classes.txt'\n",
    "result = model.inference(image=img, show=True, class_path=class_path, checkpoint = checkpoint)\n",
    "model.print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时您会发现当前目录下“cls_result”文件夹里出现了这组图片的推理结果图，名称均为原本的图片文件名称。可以得到这个文件夹下查看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.模型训练\n",
    "\n",
    "### 4.1 基于预训练模型继续训练\n",
    "\n",
    "全新开始训练一个模型，一般要花较长时间。如果使用CPU训练，需要的时间就更长了。因此我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样。\n",
    "\n",
    "- 注：如果想使用GPU环境加速训练，您可以参考我们的环境配置文档来配置环境（需要您的设备有支持CUDA加速的GPU）。\n",
    "\n",
    "#### 第一步、实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cls('LeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二步、配置基本信息\n",
    "\n",
    "至少需要提供四大基本信息：图片分类的类别数量（`model.num_classes`），新模型保存的路径（`model.save_fold`），数据集的路径（`model.load_dataset`）和预训练模型的权重文件（`checkpoint`）。\n",
    "\n",
    "`MMEdu`根目录下的`dataset`文件夹下保存了各个任务的数据集，请大家在这个文件夹中找，`hand_gray`在`cls`文件夹下，包含三个图片文件夹，`test_set`,`training_set`,`val_set`分别存储测试集，训练集和验证集的图片；以及三个`txt`文件，`classes.txt`记录该数据集的类别，`test.txt`和`val.txt`分别记录测试集和验证集的图片名。在我们的文档中有关于数据集格式的介绍。\n",
    "\n",
    "`MMEdu`根目录下的`checkpoints`文件夹，保存了很多网络的预训练模型，请大家在这个文件夹中找，在我们的文档中有关于预训练权重的介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_classes = 3\n",
    "model.load_dataset(path='../dataset/cls/hand_gray')\n",
    "model.save_fold = '../checkpoints/cls_model/hand_gray'\n",
    "checkpoint = '../checkpoints/cls_model/hand_gray/latest.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三步：开始继续训练\n",
    "\n",
    "`epochs`指训练的轮次。机器学习的训练是一个反复的过程，一般来说，需要经历一定轮次才能达到较好的效果。你可以先为改一个较小的数字，如`1`，看一下训练一轮需要多少时间，然后根据自己的需要调整`epochs`再次执行这行命令。\n",
    "\n",
    "`validate=True`表示每轮（每个epoch）训练后，在验证集（val_set）上测试一次准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:01:33,227 - mmcls - INFO - Start running, host: mi@DESKTOP-QYS, work_dir: C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray\n",
      "2022-06-09 20:01:33,229 - mmcls - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-06-09 20:01:33,230 - mmcls - INFO - workflow: [('train', 1)], max: 5 epochs\n",
      "2022-06-09 20:01:33,231 - mmcls - INFO - Checkpoints will be saved to C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ../checkpoints/cls_model/hand_gray/latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:02:06,570 - mmcls - INFO - Epoch [1][10/40]\tlr: 1.000e-02, eta: 0:10:33, time: 3.333, data_time: 3.286, loss: 0.0017\n",
      "2022-06-09 20:02:07,181 - mmcls - INFO - Epoch [1][20/40]\tlr: 1.000e-02, eta: 0:05:05, time: 0.061, data_time: 0.021, loss: 0.0016\n",
      "2022-06-09 20:02:07,895 - mmcls - INFO - Epoch [1][30/40]\tlr: 1.000e-02, eta: 0:03:16, time: 0.072, data_time: 0.014, loss: 0.0017\n",
      "2022-06-09 20:02:08,473 - mmcls - INFO - Epoch [1][40/40]\tlr: 1.000e-02, eta: 0:02:20, time: 0.058, data_time: 0.024, loss: 0.0014\n",
      "2022-06-09 20:02:08,953 - mmcls - INFO - Saving checkpoint at 1 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 1.8 task/s, elapsed: 18s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:02:27,335 - mmcls - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.\n",
      "2022-06-09 20:02:27,337 - mmcls - INFO - Best accuracy_top-1 is 84.8485 at 1 epoch.\n",
      "2022-06-09 20:02:27,339 - mmcls - INFO - Epoch(val) [1][1]\taccuracy_top-1: 84.8485\n",
      "2022-06-09 20:03:04,716 - mmcls - INFO - Epoch [2][10/40]\tlr: 1.000e-02, eta: 0:03:37, time: 3.737, data_time: 3.698, loss: 0.0014\n",
      "2022-06-09 20:03:05,227 - mmcls - INFO - Epoch [2][20/40]\tlr: 1.000e-02, eta: 0:02:50, time: 0.051, data_time: 0.015, loss: 0.0019\n",
      "2022-06-09 20:03:05,762 - mmcls - INFO - Epoch [2][30/40]\tlr: 1.000e-02, eta: 0:02:16, time: 0.053, data_time: 0.014, loss: 0.0017\n",
      "2022-06-09 20:03:06,192 - mmcls - INFO - Epoch [2][40/40]\tlr: 1.000e-02, eta: 0:01:51, time: 0.043, data_time: 0.011, loss: 0.0016\n",
      "2022-06-09 20:03:06,915 - mmcls - INFO - Saving checkpoint at 2 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 246.9 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:03:07,074 - mmcls - INFO - The previous best checkpoint C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray\\best_accuracy_top-1_epoch_1.pth was removed\n",
      "2022-06-09 20:03:07,088 - mmcls - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_2.pth.\n",
      "2022-06-09 20:03:07,090 - mmcls - INFO - Best accuracy_top-1 is 87.8788 at 2 epoch.\n",
      "2022-06-09 20:03:07,092 - mmcls - INFO - Epoch(val) [2][1]\taccuracy_top-1: 87.8788\n",
      "2022-06-09 20:03:42,856 - mmcls - INFO - Epoch [3][10/40]\tlr: 1.000e-02, eta: 0:02:14, time: 3.576, data_time: 3.542, loss: 0.0017\n",
      "2022-06-09 20:03:43,362 - mmcls - INFO - Epoch [3][20/40]\tlr: 1.000e-02, eta: 0:01:50, time: 0.050, data_time: 0.010, loss: 0.0016\n",
      "2022-06-09 20:03:43,881 - mmcls - INFO - Epoch [3][30/40]\tlr: 1.000e-02, eta: 0:01:30, time: 0.052, data_time: 0.016, loss: 0.0017\n",
      "2022-06-09 20:03:44,316 - mmcls - INFO - Epoch [3][40/40]\tlr: 1.000e-02, eta: 0:01:14, time: 0.043, data_time: 0.009, loss: 0.0015\n",
      "2022-06-09 20:03:44,892 - mmcls - INFO - Saving checkpoint at 3 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 231.4 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:03:45,056 - mmcls - INFO - Epoch(val) [3][1]\taccuracy_top-1: 87.8788\n",
      "2022-06-09 20:04:19,106 - mmcls - INFO - Epoch [4][10/40]\tlr: 1.000e-02, eta: 0:01:18, time: 3.404, data_time: 3.369, loss: 0.0016\n",
      "2022-06-09 20:04:19,634 - mmcls - INFO - Epoch [4][20/40]\tlr: 1.000e-02, eta: 0:01:02, time: 0.053, data_time: 0.014, loss: 0.0016\n",
      "2022-06-09 20:04:20,171 - mmcls - INFO - Epoch [4][30/40]\tlr: 1.000e-02, eta: 0:00:48, time: 0.054, data_time: 0.006, loss: 0.0016\n",
      "2022-06-09 20:04:20,579 - mmcls - INFO - Epoch [4][40/40]\tlr: 1.000e-02, eta: 0:00:36, time: 0.041, data_time: 0.010, loss: 0.0015\n",
      "2022-06-09 20:04:21,081 - mmcls - INFO - Saving checkpoint at 4 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 282.8 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:04:21,220 - mmcls - INFO - Epoch(val) [4][1]\taccuracy_top-1: 81.8182\n",
      "2022-06-09 20:04:53,468 - mmcls - INFO - Epoch [5][10/40]\tlr: 1.000e-02, eta: 0:00:31, time: 3.224, data_time: 3.191, loss: 0.0014\n",
      "2022-06-09 20:04:53,978 - mmcls - INFO - Epoch [5][20/40]\tlr: 1.000e-02, eta: 0:00:19, time: 0.051, data_time: 0.015, loss: 0.0017\n",
      "2022-06-09 20:04:54,470 - mmcls - INFO - Epoch [5][30/40]\tlr: 1.000e-02, eta: 0:00:09, time: 0.049, data_time: 0.019, loss: 0.0017\n",
      "2022-06-09 20:04:54,932 - mmcls - INFO - Epoch [5][40/40]\tlr: 1.000e-02, eta: 0:00:00, time: 0.046, data_time: 0.013, loss: 0.0014\n",
      "2022-06-09 20:04:55,434 - mmcls - INFO - Saving checkpoint at 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 262.6 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:04:55,581 - mmcls - INFO - Epoch(val) [5][1]\taccuracy_top-1: 81.8182\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=5, validate=True, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时在`model.save_fold`指定的`checkpoints\\cls_model`文件夹中我们会发现多了两种文件，一种是`***.log.json`日志文件，它记录了我们模型在训练过程中的一些参数，比如说学习率`lr`，所用时间`time`，损失`loss`，以及准确率`accuracy_top-1`等；另一种文件是.pth文件，这个是我们在训练过程中所保存的模型权重文件，分为按照训练轮次生成的权重文件`epoch_x.pth`和一个`best_accuracy_top-1`权重文件，`best_accuracy_top-1`权重文件即目前为止准确率最高的权重。\n",
    "\n",
    "`accuracy_top-1`：对一个图片，如果概率最大的是正确答案，才认为正确，再根据分类正确的样本数除以所有的样本数计算得到的准确率。\n",
    "\n",
    "我们可以根据`loss`和`accuracy_top-1`判断训练效果是否达到预期，如果达到预期可以停止训练，如果准确率降低应该提前停止训练，防止过拟合。就可以使用`best_accuracy_top-1`权重文件来进行推理。\n",
    "\n",
    "*过拟合（Overfitting）是值太过贴近训练数据的特征，在训练集上表现得非常优秀。近乎完美地预测或区分了所有数据，但是在新的测试集上却表现平平，不具泛化性，拿到新样本后没有办法去准确地判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 从零开始训练新模型\n",
    "\n",
    "当然，你也可以从零开始，训练一个新的模型。具体代码和`4.1`几乎一致。\n",
    "\n",
    "#### 第一步：实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cls('LeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要说明的是，`MMClassification模块`内置了好几个`SOTA模型`，如`MobileNet`、`LeNet`和`ResNet50`等。其中`LeNet`适用于单通道的图片，即灰度的图片，典型应用是手写体的识别，`MobileNet`则是RGB的图片分类最好的选择之一。要根据不同的需求，选择不同的网络模型。\n",
    "\n",
    "#### 第二步：配置基本信息\n",
    "\n",
    "基本信息有三类，分别是：图片分类的类别数量（`model.num_classes`），模型保存的路径（`model.save_fold`）和数据集的路径（`model.load_dataset`）。\n",
    "\n",
    "至于使用什么数据集，看你要解决什么问题，识别什么图片了。你可以在网上下载一些公开的数据集，也可以自己制作数据集，就是不断收集图片啦。`MMEdu`的`MMClassification`支持的数据集类型是`ImageNet`，你要按照`ImageNet格式`来建立，具体步骤请参考本教程的第`5`点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_classes = 3 \n",
    "model.load_dataset(path='../dataset/cls/hand_gray') \n",
    "model.save_fold = '../checkpoints/cls_model/hand_gray' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三步：开始训练模型\n",
    "\n",
    "同样，你也可以先把`epochs`改小一点，比如`10`。训练10轮后测试一下，效果不好再继续训练。训练的速度受CPU或者GPU的速度决定，一般来说CPU训练速度较慢，如果你的数据集图片比较多，又是全新训练的，一般都需要100多轮才会有较好的表现，你要有心理准备哦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:04:57,221 - mmcls - INFO - \n",
      "backbone.features.0.weight - torch.Size([6, 1, 5, 5]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,224 - mmcls - INFO - \n",
      "backbone.features.0.bias - torch.Size([6]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,226 - mmcls - INFO - \n",
      "backbone.features.3.weight - torch.Size([16, 6, 5, 5]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,228 - mmcls - INFO - \n",
      "backbone.features.3.bias - torch.Size([16]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,229 - mmcls - INFO - \n",
      "backbone.features.6.weight - torch.Size([120, 16, 5, 5]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,231 - mmcls - INFO - \n",
      "backbone.features.6.bias - torch.Size([120]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,233 - mmcls - INFO - \n",
      "backbone.classifier.0.weight - torch.Size([84, 120]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,235 - mmcls - INFO - \n",
      "backbone.classifier.0.bias - torch.Size([84]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,237 - mmcls - INFO - \n",
      "backbone.classifier.2.weight - torch.Size([3, 84]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,239 - mmcls - INFO - \n",
      "backbone.classifier.2.bias - torch.Size([3]): \n",
      "The value is the same before and after calling `init_weights` of ImageClassifier  \n",
      " \n",
      "2022-06-09 20:04:57,370 - mmcls - INFO - Start running, host: mi@DESKTOP-QYS, work_dir: C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray\n",
      "2022-06-09 20:04:57,373 - mmcls - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-06-09 20:04:57,376 - mmcls - INFO - workflow: [('train', 1)], max: 10 epochs\n",
      "2022-06-09 20:04:57,379 - mmcls - INFO - Checkpoints will be saved to C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray by HardDiskBackend.\n",
      "2022-06-09 20:05:29,839 - mmcls - INFO - Epoch [1][10/40]\tlr: 1.000e-02, eta: 0:21:05, time: 3.245, data_time: 3.215, loss: 1.0977\n",
      "2022-06-09 20:05:30,309 - mmcls - INFO - Epoch [1][20/40]\tlr: 1.000e-02, eta: 0:10:25, time: 0.047, data_time: 0.014, loss: 1.0951\n",
      "2022-06-09 20:05:30,801 - mmcls - INFO - Epoch [1][30/40]\tlr: 1.000e-02, eta: 0:06:52, time: 0.049, data_time: 0.018, loss: 1.0894\n",
      "2022-06-09 20:05:31,239 - mmcls - INFO - Epoch [1][40/40]\tlr: 1.000e-02, eta: 0:05:04, time: 0.044, data_time: 0.015, loss: 1.0883\n",
      "2022-06-09 20:05:31,744 - mmcls - INFO - Saving checkpoint at 1 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 1.8 task/s, elapsed: 18s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:05:49,862 - mmcls - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.\n",
      "2022-06-09 20:05:49,864 - mmcls - INFO - Best accuracy_top-1 is 42.4242 at 1 epoch.\n",
      "2022-06-09 20:05:49,866 - mmcls - INFO - Epoch(val) [1][1]\taccuracy_top-1: 42.4242\n",
      "2022-06-09 20:06:22,862 - mmcls - INFO - Epoch [2][10/40]\tlr: 1.000e-02, eta: 0:07:47, time: 3.299, data_time: 3.270, loss: 1.0585\n",
      "2022-06-09 20:06:23,342 - mmcls - INFO - Epoch [2][20/40]\tlr: 1.000e-02, eta: 0:06:21, time: 0.048, data_time: 0.018, loss: 1.0593\n",
      "2022-06-09 20:06:23,840 - mmcls - INFO - Epoch [2][30/40]\tlr: 1.000e-02, eta: 0:05:19, time: 0.050, data_time: 0.018, loss: 1.0380\n",
      "2022-06-09 20:06:24,312 - mmcls - INFO - Epoch [2][40/40]\tlr: 1.000e-02, eta: 0:04:33, time: 0.047, data_time: 0.017, loss: 1.0126\n",
      "2022-06-09 20:06:24,873 - mmcls - INFO - Saving checkpoint at 2 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 209.4 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:06:25,053 - mmcls - INFO - Epoch(val) [2][1]\taccuracy_top-1: 42.4242\n",
      "2022-06-09 20:07:00,828 - mmcls - INFO - Epoch [3][10/40]\tlr: 1.000e-02, eta: 0:05:58, time: 3.576, data_time: 3.504, loss: 0.9490\n",
      "2022-06-09 20:07:01,440 - mmcls - INFO - Epoch [3][20/40]\tlr: 1.000e-02, eta: 0:05:14, time: 0.061, data_time: 0.002, loss: 0.9164\n",
      "2022-06-09 20:07:01,933 - mmcls - INFO - Epoch [3][30/40]\tlr: 1.000e-02, eta: 0:04:37, time: 0.049, data_time: 0.002, loss: 0.8496\n",
      "2022-06-09 20:07:02,344 - mmcls - INFO - Epoch [3][40/40]\tlr: 1.000e-02, eta: 0:04:06, time: 0.041, data_time: 0.008, loss: 0.7503\n",
      "2022-06-09 20:07:02,876 - mmcls - INFO - Saving checkpoint at 3 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 238.0 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:07:03,038 - mmcls - INFO - The previous best checkpoint C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray\\best_accuracy_top-1_epoch_1.pth was removed\n",
      "2022-06-09 20:07:03,056 - mmcls - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_3.pth.\n",
      "2022-06-09 20:07:03,058 - mmcls - INFO - Best accuracy_top-1 is 60.6061 at 3 epoch.\n",
      "2022-06-09 20:07:03,062 - mmcls - INFO - Epoch(val) [3][1]\taccuracy_top-1: 60.6061\n",
      "2022-06-09 20:07:41,143 - mmcls - INFO - Epoch [4][10/40]\tlr: 1.000e-02, eta: 0:04:58, time: 3.807, data_time: 3.778, loss: 0.6190\n",
      "2022-06-09 20:07:41,638 - mmcls - INFO - Epoch [4][20/40]\tlr: 1.000e-02, eta: 0:04:27, time: 0.050, data_time: 0.017, loss: 0.5011\n",
      "2022-06-09 20:07:42,134 - mmcls - INFO - Epoch [4][30/40]\tlr: 1.000e-02, eta: 0:04:01, time: 0.050, data_time: 0.019, loss: 0.3952\n",
      "2022-06-09 20:07:42,586 - mmcls - INFO - Epoch [4][40/40]\tlr: 1.000e-02, eta: 0:03:37, time: 0.045, data_time: 0.015, loss: 0.3402\n",
      "2022-06-09 20:07:43,108 - mmcls - INFO - Saving checkpoint at 4 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 33/33, 298.1 task/s, elapsed: 0s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 20:07:43,243 - mmcls - INFO - The previous best checkpoint C:\\data\\0.6.3\\Project\\checkpoints\\cls_model\\hand_gray\\best_accuracy_top-1_epoch_3.pth was removed\n",
      "2022-06-09 20:07:43,256 - mmcls - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_4.pth.\n",
      "2022-06-09 20:07:43,258 - mmcls - INFO - Best accuracy_top-1 is 78.7879 at 4 epoch.\n",
      "2022-06-09 20:07:43,260 - mmcls - INFO - Epoch(val) [4][1]\taccuracy_top-1: 78.7879\n",
      "2022-06-09 20:08:14,239 - mmcls - INFO - Epoch [5][10/40]\tlr: 1.000e-02, eta: 0:03:58, time: 3.097, data_time: 3.062, loss: 0.2705\n",
      "2022-06-09 20:08:14,706 - mmcls - INFO - Epoch [5][20/40]\tlr: 1.000e-02, eta: 0:03:35, time: 0.047, data_time: 0.015, loss: 0.2296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\Classification_Edu.py:116\u001b[0m, in \u001b[0;36mMMClassification.train\u001b[1;34m(self, random_seed, save_fold, distributed, validate, device, metric, save_best, optimizer, epochs, lr, weight_decay, checkpoint)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mgpu_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m random_seed\n\u001b[1;32m--> 116\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocaltime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\mmcls\\apis\\train.py:205\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataset, cfg, distributed, validate, timestamp, device, meta)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mload_from:\n\u001b[0;32m    204\u001b[0m     runner\u001b[38;5;241m.\u001b[39mload_checkpoint(cfg\u001b[38;5;241m.\u001b[39mload_from)\n\u001b[1;32m--> 205\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py:127\u001b[0m, in \u001b[0;36mEpochBasedRunner.run\u001b[1;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs:\n\u001b[0;32m    126\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m             \u001b[43mepoch_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# wait for some hooks like loggers to finish\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py:51\u001b[0m, in \u001b[0;36mEpochBasedRunner.train\u001b[1;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_iter(data_batch, train_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_train_iter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\mmcv\\runner\\base_runner.py:309\u001b[0m, in \u001b[0;36mBaseRunner.call_hook\u001b[1;34m(self, fn_name)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m\"\"\"Call all hooks.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    fn_name (str): The function name in each hook to be called, such as\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m        \"before_train_epoch\".\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\mmcv\\runner\\hooks\\optimizer.py:56\u001b[0m, in \u001b[0;36mOptimizerHook.after_train_iter\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_anomalous_params:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_anomalous_parameters(runner\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], runner)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_grads(runner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\torch\\tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\data\\0.6.3\\Project\\MMEdu\\mmedu\\lib\\site-packages\\torch\\autograd\\__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(epochs=10, lr=0.01, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的，可以查看`model.save_fold`指定的文件夹中的日志文件和权重文件，根据`loss`和`accuracy_top-1`判断训练效果是否达到预期，如果达到预期可以停止训练，如果准确率降低应该提前停止训练，防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.制作一个自己的数据集\n",
    "我们已经掌握了AI训练的流程，现在你一定想知道怎么改变数据集训练自己的图片，解决分类问题吧？其中的关键点在于制作数据集。\n",
    "### 5.1 MMEdu提供了哪些数据集\n",
    "MMEdu系列提供了包括分类和检测任务的若干数据集，存储在`dataset`文件夹下。下面主要介绍分类（`cls`）数据集。\n",
    "- hand_gray\n",
    "石头剪刀布分类数据集。数据集包含2925张石头剪刀布灰度图片，分为training_set、val_set和test_set。\n",
    "- cats_dogs_dataset\n",
    "猫狗分类数据集。数据集包含7000张猫狗图片，分为training_set、val_set和test_sett。\n",
    "- minist\n",
    "手写数字数据集。数据集包含70000张0-9数字灰度图片，分为training_set、val_set和test_set。\n",
    "\n",
    "### 5.2 可以下载数据集的地方\n",
    "- [OpenMMLab数据集资源库](https://openmmlab.com/dataset)\n",
    "- [MMEdu教育团队数据集资源库](https://p6bm2if73b.feishu.cn/drive/folder/fldcnfDtXSQx0PDuUGLWZlnVO3g)\n",
    "- [Kaggle](https://www.kaggle.com/datasets)\n",
    "- [飞桨AI Studio](https://aistudio.baidu.com/aistudio/datasetoverview)\n",
    "\n",
    "### 5.3 ImageNet格式的数据集如何整理\n",
    "如果您希望从零开始整理数据集，由于生活中常见的是彩色图，所以我们将一步一步带您制作`cats_dogs_dataset`这样符合`ImageNet格式`要求的彩色图数据集。\n",
    "\n",
    "如果您是下载的数据集，也请参考这里的步骤检查数据集是否已经符合`ImageNet格式`要求。\n",
    "#### 第一步：拍摄图片\n",
    "您可以用任何设备拍摄图像，也可以从视频中抽取帧图像，需要注意，这些图像可以被划分为多个类别。\n",
    "#### 第二步：将图像按照类别划分\n",
    "每个类别建立一个文件夹，文件夹名称为类别名称，将图片放在其中。\n",
    "#### 第三步：划分训练集、验证集和测试集\n",
    "您会发现，在`cats_dogs_dataset`数据集中，图像被分在3个文件夹中，其中训练集的图片被用来训练提升模型准确性，验证集被用来验证模型的准确性，并确保模型没有过拟合，测试集被用来最终测试模型的效果。\n",
    "#### 第四步：生成标签文件\n",
    "您会发现，在`cats_dogs_dataset`数据集中，还存在3个文本文件（后缀为.txt），它们是数据集的标签文件。`classes.txt`包含数据集类别标签信息，每类数据的名称，也就是第二步中的文件夹名称，每个名称一行，按照字母顺序排列。`val.txt`是`val_set`文件夹图片与类别的对应文件说明，每行两个数据，左侧是图像文件名，空格右侧是图像的列别序号，序号与`classes.txt`中的顺序一致（从0开始）。`test.txt`与`val.txt`类似，是`test_set`文件夹图片与类别的对应文件说明，每行两个数据，左侧是图像文件名，右侧是图像的列别序号，序号与`classes.txt`中的顺序一致（从0开始）。\n",
    "#### 第五步：给数据集命名\n",
    "最后，我们将这些文件放在一个文件夹中，命名为数据集的名称。这样，在训练的时候，只要通过`model.load_dataset`指定数据集的路径就可以了。\n",
    "\n",
    "#### 参考文档\n",
    "[MMEdu的数据集格式详解](https://mmedu.readthedocs.io/zh/latest/%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B/MMEdu%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
